{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1234)\n",
    "threhold = 0.357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append\n"
     ]
    }
   ],
   "source": [
    "dataset_path= \"/mnt/c/Users/didi/Downloads/RainNet_HDF5\"      # path to RainNet HDF5 file\n",
    "patch_hdf5_root   = \"/mnt/c/Users/didi/Downloads/RainNet_HDF5\" # root patch to save patches\n",
    "\n",
    "if not os.path.exists(patch_hdf5_root):\n",
    "    os.makedirs(patch_hdf5_root)\n",
    "hdf5_path   = os.path.join(patch_hdf5_root, \"RainNet_Patches.hdf5\")\n",
    "\n",
    "month_list  = [\"07\",\"08\",\"09\",\"10\",\"11\"]\n",
    "year_start  = 2002\n",
    "\n",
    "t_stride    = 4\n",
    "frame       = 5\n",
    "patch_size  = 64\n",
    "patch_per_img=4\n",
    "\n",
    "ratio       = 3\n",
    "HR_patch    = 64 * ratio\n",
    "max_rain    = 140.0\n",
    "\n",
    "high_list   = []\n",
    "low_list    = []\n",
    "\n",
    "total_patch = 0\n",
    "\n",
    "if not os.path.exists(hdf5_path):\n",
    "    h5file = h5py.File(hdf5_path, 'w')\n",
    "    print(\"write\")\n",
    "else:\n",
    "    h5file = h5py.File(hdf5_path, 'a')\n",
    "    print(\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RainNet_Patches.hdf5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%s.hdf5\"%\"RainNet_Patches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(dataset_path + f\"/{2017}_08.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"2017_08.hdf5\" (mode r)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"/home/diler/dev/downscaling/dataset/rainnet/2017_08.hdf5\"\n",
    "h5py.File(file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/c/Users/didi/Downloads/RainNet_HDF5/2017_08.hdf5......\n",
      "Processing hdf5 file: /mnt/c/Users/didi/Downloads/RainNet_HDF5/2017_08.hdf5\n",
      "#===========One Batch==========#\n"
     ]
    }
   ],
   "source": [
    "year_item = 2017\n",
    "tail = \"08\"\n",
    "\n",
    "i_path      = dataset_path + f\"/{year_item}_{tail}.hdf5\"\n",
    "print(\"Processing %s......\"%i_path)\n",
    "f           = h5py.File(i_path, 'r')\n",
    "high_list   = f['highres']\n",
    "low_list    = f['lowres']\n",
    "print(\"Processing hdf5 file: %s\"%i_path)\n",
    "total_len   = low_list.shape[2]\n",
    "hight       = low_list.shape[0]\n",
    "width       = low_list.shape[1]\n",
    "print(\"#===========One Batch==========#\")\n",
    "for index in range(0, total_len-frame, t_stride):\n",
    "    count = 0\n",
    "    for _ in range(20):\n",
    "        r_h   = random.randint(0,hight-patch_size)\n",
    "        r_w   = random.randint(0,width-patch_size)\n",
    "        lr_patch = low_list[r_h:(r_h+patch_size),r_w:(r_w+patch_size),index]\n",
    "        if lr_patch.max() > threhold: # leverage hr patch\n",
    "            hr_patch_ls = high_list[r_h * ratio:(r_h * ratio + HR_patch),\n",
    "                                    r_w * ratio:(r_w * ratio + HR_patch), index:(index + frame//2)]\n",
    "            lr_patch_ls = low_list[r_h:(r_h+patch_size),r_w:(r_w+patch_size), index:(index + frame//2)]\n",
    "            hr_patch_ls = np.clip(hr_patch_ls.transpose((2,0,1)), 0.0, max_rain)/max_rain\n",
    "            lr_patch_ls = np.clip(lr_patch_ls.transpose((2,0,1)), 0.0, max_rain)/max_rain\n",
    "            h5file.create_dataset(str(total_patch)+\"hr\", data=hr_patch_ls)\n",
    "            h5file.create_dataset(str(total_patch)+\"lr\", data=lr_patch_ls)\n",
    "            count       += 1\n",
    "            total_patch += 1\n",
    "        if count >patch_per_img:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "downscaling-hfnw0hYX-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
